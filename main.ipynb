{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import robosuite as suite\n",
    "from robosuite.wrappers import GymWrapper\n",
    "from td3_torch import Agent\n",
    "from buffer import ReplayBuffer\n",
    "from networks import CriticNetwork, ActorNetwork\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists('tmp/td3'):\n",
    "        os.makedirs('tmp/td3')\n",
    "\n",
    "    env_name = \"Door\"\n",
    "    env = suite.make(\n",
    "        env_name,\n",
    "        robots=[\"Panda\"],\n",
    "        controller_configs=suite.load_controller_config(default_controller=\"JOINT_VELOCITY\"),\n",
    "        has_renderer=False,\n",
    "        use_camera_obs=False,\n",
    "        horizon=300,\n",
    "        reward_shaping=True,\n",
    "        control_freq=20\n",
    "    )\n",
    "\n",
    "    env = GymWrapper(env)\n",
    "    print(env.observation_space.shape[0])\n",
    "\n",
    "    actor_learning_rate=0.001\n",
    "    critic_learning_rate=0.001\n",
    "    batch_size=128\n",
    "    layer1_size=256\n",
    "    layer2_size=128\n",
    "\n",
    "    agent = Agent(actor_learning_rate=actor_learning_rate,critic_learning_rate=critic_learning_rate,tau = 0.005, input_dims=env.observation_space.shape,\n",
    "                  env=env,n_actions=env.action_space.shape[0], layer1_size=layer1_size, layer2_size=layer2_size, batch_size=batch_size)\n",
    "\n",
    "    writer = SummaryWriter('logs')\n",
    "    n_games = 3\n",
    "\n",
    "    for experiment in range(10):\n",
    "        best_score = 0\n",
    "        episode_identifier = f\"{experiment} - actor_learning_rate={actor_learning_rate} critic_learning_rate={critic_learning_rate} layer1_size={layer1_size} layer2_size={layer2_size}\"\n",
    "\n",
    "        #agent.load_model()\n",
    "\n",
    "        for i in range(n_games):\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            score = 0.0\n",
    "\n",
    "            while not done:\n",
    "                action = agent.choose_action(observation)\n",
    "                next_observation, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                agent.remember(observation, action, reward, next_observation, done)\n",
    "                agent.learn()\n",
    "                observation = next_observation\n",
    "\n",
    "            writer.add_scalar(f\"Score - {episode_identifier}\", score, global_step=i)\n",
    "\n",
    "            if(i % 10):\n",
    "                agent.save_model()\n",
    "\n",
    "            print(f\"Episode: {i}, Score: {score}\")\n",
    "\n",
    "            actor_learning_rate = actor_learning_rate + 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
